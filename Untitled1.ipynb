{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16604352-6a4b-4856-83e2-c063adb4143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dd6f58e-3c8f-427e-9a5a-5948ee147b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = [(\"libro1\",\"HELLO WORL! This is our  first @ document...\"),\n",
    "         (\"libro2\",\"I want to starT learnign something about me.\"),\n",
    "         (\"libro3\",\"This is gonna be a book about monsters and teenagers!!!\")]\n",
    "schema = ['nombre','contenido']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2cc03bc-9ba3-47aa-8fe5-112043ea6d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Dataframes\") \\\n",
    "        .master('local[*]') \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4c12eaf-0c3e-4cf1-8965-39b88b9c6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(datos, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff55e521-c57b-4513-ba3a-3cfa125ac2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|nombre|           contenido|\n",
      "+------+--------------------+\n",
      "|libro1|HELLO WORL! This ...|\n",
      "|libro2|I want to starT l...|\n",
      "|libro3|This is gonna be ...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dfe7d7a-2de0-457d-a54d-fce7a8f434c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------------------------------+\n",
      "|nombre|contenido                                              |\n",
      "+------+-------------------------------------------------------+\n",
      "|libro1|HELLO WORL! This is our  first @ document...           |\n",
      "|libro2|I want to starT learnign something about me.           |\n",
      "|libro3|This is gonna be a book about monsters and teenagers!!!|\n",
      "+------+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0de786a1-6a45-4bfa-a38a-7f2a5696f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, lower, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc6aeb96-e3de-468b-9162-9da43fe779fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('contenido', lower(df.contenido))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71646f09-e5db-42c9-a1cb-db4dcd398e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------------------------------+\n",
      "|nombre|contenido                                              |\n",
      "+------+-------------------------------------------------------+\n",
      "|libro1|hello worl! this is our  first @ document...           |\n",
      "|libro2|i want to start learnign something about me.           |\n",
      "|libro3|this is gonna be a book about monsters and teenagers!!!|\n",
      "+------+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8f661ee-8290-4246-974e-384cb19f8271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1ee8bd1-e124-45e0-b2e1-b4f923dc8f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado = df.withColumn('contenido', regexp_replace('contenido', '[^a-zA-Z0-9-\\\\s]',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb4f4ccc-9ecc-456e-b3b9-655f93c46872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------------------+\n",
      "|nombre|contenido                                           |\n",
      "+------+----------------------------------------------------+\n",
      "|libro1|hello worl this is our  first  document             |\n",
      "|libro2|i want to start learnign something about me         |\n",
      "|libro3|this is gonna be a book about monsters and teenagers|\n",
      "+------+----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtrado.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6e0a685-4366-46c8-bd3a-c0ebf36ef107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, lower, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d121cab-c32f-40ea-b226-3e50a741ec63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|   palabras|\n",
      "+-----------+\n",
      "|      hello|\n",
      "|      worl!|\n",
      "|       this|\n",
      "|         is|\n",
      "|        our|\n",
      "|           |\n",
      "|      first|\n",
      "|document...|\n",
      "|          i|\n",
      "|       want|\n",
      "|         to|\n",
      "|      start|\n",
      "|   learnign|\n",
      "|  something|\n",
      "|      about|\n",
      "|        me.|\n",
      "|       this|\n",
      "|         is|\n",
      "|      gonna|\n",
      "|         be|\n",
      "+-----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.select( explode( split( lower(df.contenido), \" \") ).alias(\"palabras\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c176c57-41cc-41c0-9ab6-161ff6c4b416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| palabras|\n",
      "+---------+\n",
      "|    hello|\n",
      "|     worl|\n",
      "|     this|\n",
      "|       is|\n",
      "|      our|\n",
      "|         |\n",
      "|    first|\n",
      "| document|\n",
      "|        i|\n",
      "|     want|\n",
      "|       to|\n",
      "|    start|\n",
      "| learnign|\n",
      "|something|\n",
      "|    about|\n",
      "|       me|\n",
      "|     this|\n",
      "|       is|\n",
      "|    gonna|\n",
      "|       be|\n",
      "+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_filtrado.select( explode( split(df_filtrado.contenido, \" \") ).alias(\"palabras\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85e80c12-bb12-4d08-94c6-37c0c58e75c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jenoe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec6ec401-ab1d-44bd-8332-56798ceddf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jenoe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f803b4a4-bde3-4fa7-8814-0a53753e056e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer, StopWordsRemover, CountVectorizer\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m      3\u001b[39m nltk.download(\u001b[33m'\u001b[39m\u001b[33mstopwords\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0861c856-a34e-4c1b-ad91-6fd6c5617437",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer, StopWordsRemover, CountVectorizer\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m      3\u001b[39m nltk.download(\u001b[33m'\u001b[39m\u001b[33mstopwords\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b3d113e-c91f-4c10-9c1f-4b58545e89be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jenoe/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70fd4ac8-53be-4635-bdcf-11b65dec8302",
   "metadata": {},
   "outputs": [],
   "source": [
    "filterWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da5f8b87-d970-4e0c-a7ce-8fe0318ad146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "df_filtrado = df_filtrado.withColumn(\"contenido\",sf.regexp_replace(\"contenido\",\"\\\\s+\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9112d255-8eb6-42d6-b674-3bc45e627f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------------------+\n",
      "|nombre|contenido                                           |\n",
      "+------+----------------------------------------------------+\n",
      "|libro1|hello worl this is our first document               |\n",
      "|libro2|i want to start learnign something about me         |\n",
      "|libro3|this is gonna be a book about monsters and teenagers|\n",
      "+------+----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtrado.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b16d611c-169c-4e6c-b39c-ff51e3cd0972",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado = df_filtrado.withColumn(\"tokens\",sf.split(sf.col(\"contenido\"), \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb87cce4-0c1b-40ea-9e5c-697c1e9b12a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------------------+---------------------------------------------------------------+\n",
      "|nombre|contenido                                           |tokens                                                         |\n",
      "+------+----------------------------------------------------+---------------------------------------------------------------+\n",
      "|libro1|hello worl this is our first document               |[hello, worl, this, is, our, first, document]                  |\n",
      "|libro2|i want to start learnign something about me         |[i, want, to, start, learnign, something, about, me]           |\n",
      "|libro3|this is gonna be a book about monsters and teenagers|[this, is, gonna, be, a, book, about, monsters, and, teenagers]|\n",
      "+------+----------------------------------------------------+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtrado.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17f728e6-0a52-4960-8f9c-47fa9d1cc64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado = df_filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8b317c0-f4fd-44e3-8c1a-85a7d7056986",
   "metadata": {},
   "outputs": [],
   "source": [
    "filterWordsCol = sf.lit(filterWords)\n",
    "df_filtrado = df_filtrado.withColumn(\"tokens\",sf.filter(sf.col(\"tokens\"), \n",
    "                                                             lambda x: ~sf.array_contains\n",
    "                                                             (filterWordsCol, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "890a2f4f-f6b4-429a-8255-0a8a9ffa5cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------------------+----------------------------------+\n",
      "|nombre|contenido                                           |tokens                            |\n",
      "+------+----------------------------------------------------+----------------------------------+\n",
      "|libro1|hello worl this is our first document               |[hello, worl, first, document]    |\n",
      "|libro2|i want to start learnign something about me         |[want, start, learnign, something]|\n",
      "|libro3|this is gonna be a book about monsters and teenagers|[gonna, book, monsters, teenagers]|\n",
      "+------+----------------------------------------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtrado.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e19fabe-68ce-49d4-b9a9-0c772478058f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/05 21:03:43 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "{\"ts\": \"2025-12-05 21:03:43.946\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id` cannot be resolved. Did you mean one of the following? [`words`, `nombre`, `tokens`, `contenido`]. SQLSTATE: 42703\", \"context\": {\"file\": \"java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o315.select.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id` cannot be resolved. Did you mean one of the following? [`words`, `nombre`, `tokens`, `contenido`]. SQLSTATE: 42703;\\n'Project ['id, words#89]\\n+- Generate explode(tokens#76), false, [words#89]\\n   +- Project [nombre#25, contenido#57, filter(tokens#65, lambdafunction(NOT array_contains(array(a, about, above, after, again, against, ain, all, am, an, and, any, are, aren, aren't, as, at, be, because, been, before, being, below, between, both, ... 173 more fields), lambda x_1#77), lambda x_1#77, false)) AS tokens#76]\\n      +- Project [nombre#25, contenido#57, split(contenido#57,  , -1) AS tokens#65]\\n         +- Project [nombre#25, regexp_replace(contenido#49, \\\\s+,  , 1) AS contenido#57]\\n            +- Project [nombre#25, regexp_replace(contenido#41, [^a-zA-Z0-9-\\\\s], , 1) AS contenido#49]\\n               +- Project [nombre#25, lower(contenido#26) AS contenido#41]\\n                  +- LogicalRDD [nombre#25, contenido#26], false\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2263)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:894)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:232)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:1583)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 21 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/home/jenoe/proyecto/venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/home/jenoe/proyecto/venv/lib/python3.12/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id` cannot be resolved. Did you mean one of the following? [`words`, `nombre`, `tokens`, `contenido`]. SQLSTATE: 42703;\n'Project ['id, words#89]\n+- Generate explode(tokens#76), false, [words#89]\n   +- Project [nombre#25, contenido#57, filter(tokens#65, lambdafunction(NOT array_contains(array(a, about, above, after, again, against, ain, all, am, an, and, any, are, aren, aren't, as, at, be, because, been, before, being, below, between, both, ... 173 more fields), lambda x_1#77), lambda x_1#77, false)) AS tokens#76]\n      +- Project [nombre#25, contenido#57, split(contenido#57,  , -1) AS tokens#65]\n         +- Project [nombre#25, regexp_replace(contenido#49, \\s+,  , 1) AS contenido#57]\n            +- Project [nombre#25, regexp_replace(contenido#41, [^a-zA-Z0-9-\\s], , 1) AS contenido#49]\n               +- Project [nombre#25, lower(contenido#26) AS contenido#41]\n                  +- LogicalRDD [nombre#25, contenido#26], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_filtrado = \u001b[43mdf_filtrado\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexplode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwords\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:991\u001b[39m, in \u001b[36mDataFrame.select\u001b[39m\u001b[34m(self, *cols)\u001b[39m\n\u001b[32m    990\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cols: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> ParentDataFrame:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    992\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id` cannot be resolved. Did you mean one of the following? [`words`, `nombre`, `tokens`, `contenido`]. SQLSTATE: 42703;\n'Project ['id, words#89]\n+- Generate explode(tokens#76), false, [words#89]\n   +- Project [nombre#25, contenido#57, filter(tokens#65, lambdafunction(NOT array_contains(array(a, about, above, after, again, against, ain, all, am, an, and, any, are, aren, aren't, as, at, be, because, been, before, being, below, between, both, ... 173 more fields), lambda x_1#77), lambda x_1#77, false)) AS tokens#76]\n      +- Project [nombre#25, contenido#57, split(contenido#57,  , -1) AS tokens#65]\n         +- Project [nombre#25, regexp_replace(contenido#49, \\s+,  , 1) AS contenido#57]\n            +- Project [nombre#25, regexp_replace(contenido#41, [^a-zA-Z0-9-\\s], , 1) AS contenido#49]\n               +- Project [nombre#25, lower(contenido#26) AS contenido#41]\n                  +- LogicalRDD [nombre#25, contenido#26], false\n"
     ]
    }
   ],
   "source": [
    "df_filtrado = df_filtrado.select(\"id\",sf.explode(\"tokens\").alias(\"words\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa2801b3-d41d-4de4-a405-4a489ea4148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-12-05 21:17:38.235\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `tokens` cannot be resolved. Did you mean one of the following? [`words`, `nombre`]. SQLSTATE: 42703\", \"context\": {\"file\": \"java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o340.select.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `tokens` cannot be resolved. Did you mean one of the following? [`words`, `nombre`]. SQLSTATE: 42703;\\n'Project [nombre#25, 'explode('tokens) AS words#99]\\n+- Project [nombre#25, words#91]\\n   +- Generate explode(tokens#76), false, [words#91]\\n      +- Project [nombre#25, contenido#57, filter(tokens#65, lambdafunction(NOT array_contains(array(a, about, above, after, again, against, ain, all, am, an, and, any, are, aren, aren't, as, at, be, because, been, before, being, below, between, both, ... 173 more fields), lambda x_1#77), lambda x_1#77, false)) AS tokens#76]\\n         +- Project [nombre#25, contenido#57, split(contenido#57,  , -1) AS tokens#65]\\n            +- Project [nombre#25, regexp_replace(contenido#49, \\\\s+,  , 1) AS contenido#57]\\n               +- Project [nombre#25, regexp_replace(contenido#41, [^a-zA-Z0-9-\\\\s], , 1) AS contenido#49]\\n                  +- Project [nombre#25, lower(contenido#26) AS contenido#41]\\n                     +- LogicalRDD [nombre#25, contenido#26], false\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\\n\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2263)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:894)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:232)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:1583)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\\n\\t\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\\n\\t\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 21 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/home/jenoe/proyecto/venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/home/jenoe/proyecto/venv/lib/python3.12/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `tokens` cannot be resolved. Did you mean one of the following? [`words`, `nombre`]. SQLSTATE: 42703;\n'Project [nombre#25, 'explode('tokens) AS words#99]\n+- Project [nombre#25, words#91]\n   +- Generate explode(tokens#76), false, [words#91]\n      +- Project [nombre#25, contenido#57, filter(tokens#65, lambdafunction(NOT array_contains(array(a, about, above, after, again, against, ain, all, am, an, and, any, are, aren, aren't, as, at, be, because, been, before, being, below, between, both, ... 173 more fields), lambda x_1#77), lambda x_1#77, false)) AS tokens#76]\n         +- Project [nombre#25, contenido#57, split(contenido#57,  , -1) AS tokens#65]\n            +- Project [nombre#25, regexp_replace(contenido#49, \\s+,  , 1) AS contenido#57]\n               +- Project [nombre#25, regexp_replace(contenido#41, [^a-zA-Z0-9-\\s], , 1) AS contenido#49]\n                  +- Project [nombre#25, lower(contenido#26) AS contenido#41]\n                     +- LogicalRDD [nombre#25, contenido#26], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_filtrado = \u001b[43mdf_filtrado\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnombre\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexplode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwords\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:991\u001b[39m, in \u001b[36mDataFrame.select\u001b[39m\u001b[34m(self, *cols)\u001b[39m\n\u001b[32m    990\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cols: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> ParentDataFrame:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    992\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `tokens` cannot be resolved. Did you mean one of the following? [`words`, `nombre`]. SQLSTATE: 42703;\n'Project [nombre#25, 'explode('tokens) AS words#99]\n+- Project [nombre#25, words#91]\n   +- Generate explode(tokens#76), false, [words#91]\n      +- Project [nombre#25, contenido#57, filter(tokens#65, lambdafunction(NOT array_contains(array(a, about, above, after, again, against, ain, all, am, an, and, any, are, aren, aren't, as, at, be, because, been, before, being, below, between, both, ... 173 more fields), lambda x_1#77), lambda x_1#77, false)) AS tokens#76]\n         +- Project [nombre#25, contenido#57, split(contenido#57,  , -1) AS tokens#65]\n            +- Project [nombre#25, regexp_replace(contenido#49, \\s+,  , 1) AS contenido#57]\n               +- Project [nombre#25, regexp_replace(contenido#41, [^a-zA-Z0-9-\\s], , 1) AS contenido#49]\n                  +- Project [nombre#25, lower(contenido#26) AS contenido#41]\n                     +- LogicalRDD [nombre#25, contenido#26], false\n"
     ]
    }
   ],
   "source": [
    "df_filtrado = df_filtrado.select(\"nombre\",sf.explode(\"tokens\").alias(\"words\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d507915-edbb-4c96-88dd-eb617b6294ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00f481df-ac90-45c9-b29b-4824eb721faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "docWordCount = df_filtrado.groupBy(\"nombre\",\"words\").agg(sf.count(\"words\").alias(\"TF\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f6ca3d2-b12f-4881-bf6e-aee1ea8f802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|nombre|words    |\n",
      "+------+---------+\n",
      "|libro1|hello    |\n",
      "|libro1|worl     |\n",
      "|libro1|first    |\n",
      "|libro1|document |\n",
      "|libro2|want     |\n",
      "|libro2|start    |\n",
      "|libro2|learnign |\n",
      "|libro2|something|\n",
      "|libro3|gonna    |\n",
      "|libro3|book     |\n",
      "|libro3|monsters |\n",
      "|libro3|teenagers|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtrado.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0ddfb5a-9697-4c3e-834d-6bd177f3ea51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+\n",
      "|nombre|   words| TF|\n",
      "+------+--------+---+\n",
      "|libro1|   first|  1|\n",
      "|libro1|document|  1|\n",
      "|libro1|   hello|  1|\n",
      "|libro1|    worl|  1|\n",
      "|libro2|   start|  1|\n",
      "+------+--------+---+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(nombre='libro2', words='something', TF=1),\n",
       " Row(nombre='libro3', words='book', TF=1),\n",
       " Row(nombre='libro3', words='teenagers', TF=1),\n",
       " Row(nombre='libro3', words='monsters', TF=1),\n",
       " Row(nombre='libro3', words='gonna', TF=1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docWordCount.show(5)\n",
    "docWordCount.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3e03c73-34c6-42e8-adfc-af9f1fb7e2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "docTF = docWordCount.groupBy(\"words\").pivot('nombre').sum('tf')\n",
    "docTF = docTF.fillna(0)\n",
    "docTFOrd = docTF.orderBy(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97b8165b-eb3a-460b-af9a-b07cc43d4745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+\n",
      "|    words|libro1|libro2|libro3|\n",
      "+---------+------+------+------+\n",
      "|     book|     0|     0|     1|\n",
      "| document|     1|     0|     0|\n",
      "|    first|     1|     0|     0|\n",
      "|    gonna|     0|     0|     1|\n",
      "|    hello|     1|     0|     0|\n",
      "| learnign|     0|     1|     0|\n",
      "| monsters|     0|     0|     1|\n",
      "|something|     0|     1|     0|\n",
      "|    start|     0|     1|     0|\n",
      "|teenagers|     0|     0|     1|\n",
      "|     want|     0|     1|     0|\n",
      "|     worl|     1|     0|     0|\n",
      "+---------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docTFOrd.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f80f6ceb-b61e-403e-9887-0938f02b79ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "docCols = [x for x in docTFOrd.columns if x != 'words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0977065d-dac0-4dbb-b21c-12387ce111bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['libro1', 'libro2', 'libro3']\n"
     ]
    }
   ],
   "source": [
    "print(docCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3680528-d603-4846-b19a-fe6ba717154d",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got generator.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkTypeError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m docDF = docTFOrd.withColumn(\u001b[33m\"\u001b[39m\u001b[33mDF\u001b[39m\u001b[33m\"\u001b[39m,\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mint\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocCols\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/utils.py:282\u001b[39m, in \u001b[36mtry_remote_functions.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f.\u001b[34m__name__\u001b[39m)(*args, **kwargs)\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/functions/builtin.py:1625\u001b[39m, in \u001b[36msum\u001b[39m\u001b[34m(col)\u001b[39m\n\u001b[32m   1570\u001b[39m \u001b[38;5;129m@_try_remote_functions\u001b[39m\n\u001b[32m   1571\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msum\u001b[39m(col: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> Column:\n\u001b[32m   1572\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1573\u001b[39m \u001b[33;03m    Aggregate function: returns the sum of all values in the expression.\u001b[39;00m\n\u001b[32m   1574\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1623\u001b[39m \u001b[33;03m    +--------+\u001b[39;00m\n\u001b[32m   1624\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1625\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_invoke_function_over_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msum\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/functions/builtin.py:117\u001b[39m, in \u001b[36m_invoke_function_over_columns\u001b[39m\u001b[34m(name, *cols)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03mInvokes n-ary JVM function identified by name\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03mand wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolumn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _to_java_column\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(name, *(_to_java_column(col) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/functions/builtin.py:117\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03mInvokes n-ary JVM function identified by name\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03mand wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolumn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _to_java_column\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(name, *(\u001b[43m_to_java_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/classic/column.py:71\u001b[39m, in \u001b[36m_to_java_column\u001b[39m\u001b[34m(col)\u001b[39m\n\u001b[32m     69\u001b[39m     jcol = _create_column_from_name(col)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m     72\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN_OR_STR\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     73\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col).\u001b[34m__name__\u001b[39m},\n\u001b[32m     74\u001b[39m     )\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m jcol\n",
      "\u001b[31mPySparkTypeError\u001b[39m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got generator."
     ]
    }
   ],
   "source": [
    "docDF = docTFOrd.withColumn(\"DF\",sum((sf.col(x) > 0).cast(\"int\") for x in docCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "113ec299-ba9b-4cd3-af6b-ec44941abaa0",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got generator.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkTypeError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m docDF = docTFOrd.withColumn(\u001b[33m\"\u001b[39m\u001b[33mDF\u001b[39m\u001b[33m\"\u001b[39m,\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mint\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocCols\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/utils.py:282\u001b[39m, in \u001b[36mtry_remote_functions.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f.\u001b[34m__name__\u001b[39m)(*args, **kwargs)\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/functions/builtin.py:1625\u001b[39m, in \u001b[36msum\u001b[39m\u001b[34m(col)\u001b[39m\n\u001b[32m   1570\u001b[39m \u001b[38;5;129m@_try_remote_functions\u001b[39m\n\u001b[32m   1571\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msum\u001b[39m(col: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> Column:\n\u001b[32m   1572\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1573\u001b[39m \u001b[33;03m    Aggregate function: returns the sum of all values in the expression.\u001b[39;00m\n\u001b[32m   1574\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1623\u001b[39m \u001b[33;03m    +--------+\u001b[39;00m\n\u001b[32m   1624\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1625\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_invoke_function_over_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msum\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/functions/builtin.py:117\u001b[39m, in \u001b[36m_invoke_function_over_columns\u001b[39m\u001b[34m(name, *cols)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03mInvokes n-ary JVM function identified by name\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03mand wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolumn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _to_java_column\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(name, *(_to_java_column(col) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/functions/builtin.py:117\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03mInvokes n-ary JVM function identified by name\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03mand wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolumn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _to_java_column\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(name, *(\u001b[43m_to_java_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/classic/column.py:71\u001b[39m, in \u001b[36m_to_java_column\u001b[39m\u001b[34m(col)\u001b[39m\n\u001b[32m     69\u001b[39m     jcol = _create_column_from_name(col)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m     72\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN_OR_STR\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     73\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col).\u001b[34m__name__\u001b[39m},\n\u001b[32m     74\u001b[39m     )\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m jcol\n",
      "\u001b[31mPySparkTypeError\u001b[39m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got generator."
     ]
    }
   ],
   "source": [
    "docDF = docTFOrd.withColumn(\"DF\",sum((sf.col(x) > 0).cast(\"int\") for x in docCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce0fd789-19c4-4555-b85b-5d3e2807122d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdocDF\u001b[49m.show(\u001b[32m100\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'docDF' is not defined"
     ]
    }
   ],
   "source": [
    "docDF.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cddb8c70-c9fe-4b72-87fc-a0dae4150033",
   "metadata": {},
   "outputs": [],
   "source": [
    "docDf = docTFOrd.groupBy('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9f53943-764f-4a03-91f5-cce22cf89963",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GroupedData' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdocDf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m(\u001b[32m100\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'GroupedData' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "docDf.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b404ac2-df05-4f8a-ab86-75547273ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "docDf = docTFOrd.groupBy('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "daba3829-5f2c-4c79-b305-411240b0caa4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GroupedData' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdocDf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m(\u001b[32m100\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'GroupedData' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "docDf.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27458d40-25ab-4b62-ad54-a2d5b7d637f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GroupedData[grouping expressions: [words], value: [words: string, libro1: bigint ... 2 more fields], type: GroupBy]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docTFOrd.groupBy('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9b14a13-3bfb-4835-a746-99fc671d0d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:====================================>                     (5 + 3) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+\n",
      "|    words|libro1|libro2|libro3|\n",
      "+---------+------+------+------+\n",
      "|     book|     0|     0|     1|\n",
      "| document|     1|     0|     0|\n",
      "|    first|     1|     0|     0|\n",
      "|    gonna|     0|     0|     1|\n",
      "|    hello|     1|     0|     0|\n",
      "| learnign|     0|     1|     0|\n",
      "| monsters|     0|     0|     1|\n",
      "|something|     0|     1|     0|\n",
      "|    start|     0|     1|     0|\n",
      "|teenagers|     0|     0|     1|\n",
      "|     want|     0|     1|     0|\n",
      "|     worl|     1|     0|     0|\n",
      "+---------+------+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "docTFOrd.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a4434b9b-6c7b-4b11-a105-a9b986370f85",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN] Argument `col` should be a Column, got DataFrame.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkTypeError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m docDF = \u001b[43mdocTFOrd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDF\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocTFOrd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwords\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:1619\u001b[39m, in \u001b[36mDataFrame.withColumn\u001b[39m\u001b[34m(self, colName, col)\u001b[39m\n\u001b[32m   1617\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwithColumn\u001b[39m(\u001b[38;5;28mself\u001b[39m, colName: \u001b[38;5;28mstr\u001b[39m, col: Column) -> ParentDataFrame:\n\u001b[32m   1618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[32m-> \u001b[39m\u001b[32m1619\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   1620\u001b[39m             errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1621\u001b[39m             messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col).\u001b[34m__name__\u001b[39m},\n\u001b[32m   1622\u001b[39m         )\n\u001b[32m   1623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m._jdf.withColumn(colName, col._jc), \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[31mPySparkTypeError\u001b[39m: [NOT_COLUMN] Argument `col` should be a Column, got DataFrame."
     ]
    }
   ],
   "source": [
    "docDF = docTFOrd.withColumn(\"DF\", docTFOrd.groupBy('words').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc2b1f88-ea04-43b6-9390-789ea9960f8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataFrame.count() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m docDF = docTFOrd.withColumn(\u001b[33m\"\u001b[39m\u001b[33mDF\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mdocTFOrd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m==\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mTypeError\u001b[39m: DataFrame.count() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "docDF = docTFOrd.withColumn(\"DF\", docTFOrd.count(lambda x: x==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ebf86e9a-77a3-47a3-9492-30ffef6f79ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got generator.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkTypeError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m docDF = docTFOrd.withColumn(\u001b[33m\"\u001b[39m\u001b[33mDF\u001b[39m\u001b[33m\"\u001b[39m,\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mint\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocCols\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/utils.py:282\u001b[39m, in \u001b[36mtry_remote_functions.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f.\u001b[34m__name__\u001b[39m)(*args, **kwargs)\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/functions/builtin.py:1625\u001b[39m, in \u001b[36msum\u001b[39m\u001b[34m(col)\u001b[39m\n\u001b[32m   1570\u001b[39m \u001b[38;5;129m@_try_remote_functions\u001b[39m\n\u001b[32m   1571\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msum\u001b[39m(col: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> Column:\n\u001b[32m   1572\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1573\u001b[39m \u001b[33;03m    Aggregate function: returns the sum of all values in the expression.\u001b[39;00m\n\u001b[32m   1574\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1623\u001b[39m \u001b[33;03m    +--------+\u001b[39;00m\n\u001b[32m   1624\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1625\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_invoke_function_over_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msum\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/functions/builtin.py:117\u001b[39m, in \u001b[36m_invoke_function_over_columns\u001b[39m\u001b[34m(name, *cols)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03mInvokes n-ary JVM function identified by name\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03mand wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolumn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _to_java_column\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(name, *(_to_java_column(col) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/functions/builtin.py:117\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03mInvokes n-ary JVM function identified by name\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03mand wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolumn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _to_java_column\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(name, *(\u001b[43m_to_java_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/classic/column.py:71\u001b[39m, in \u001b[36m_to_java_column\u001b[39m\u001b[34m(col)\u001b[39m\n\u001b[32m     69\u001b[39m     jcol = _create_column_from_name(col)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m     72\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN_OR_STR\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     73\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col).\u001b[34m__name__\u001b[39m},\n\u001b[32m     74\u001b[39m     )\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m jcol\n",
      "\u001b[31mPySparkTypeError\u001b[39m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got generator."
     ]
    }
   ],
   "source": [
    "docDF = docTFOrd.withColumn(\"DF\",sum((sf.col(x) > 0).cast(\"int\") for x in docCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "907adadb-6454-4b1a-a440-ac5906c2fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "acb5ca98-dc63-4a18-b2f7-05c56cb049b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got generator.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkTypeError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m docDF = docTFOrd.withColumn(\u001b[33m\"\u001b[39m\u001b[33mDF\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocCols\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/utils.py:282\u001b[39m, in \u001b[36mtry_remote_functions.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f.\u001b[34m__name__\u001b[39m)(*args, **kwargs)\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/functions/builtin.py:1625\u001b[39m, in \u001b[36msum\u001b[39m\u001b[34m(col)\u001b[39m\n\u001b[32m   1570\u001b[39m \u001b[38;5;129m@_try_remote_functions\u001b[39m\n\u001b[32m   1571\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msum\u001b[39m(col: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> Column:\n\u001b[32m   1572\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1573\u001b[39m \u001b[33;03m    Aggregate function: returns the sum of all values in the expression.\u001b[39;00m\n\u001b[32m   1574\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1623\u001b[39m \u001b[33;03m    +--------+\u001b[39;00m\n\u001b[32m   1624\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1625\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_invoke_function_over_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msum\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/functions/builtin.py:117\u001b[39m, in \u001b[36m_invoke_function_over_columns\u001b[39m\u001b[34m(name, *cols)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03mInvokes n-ary JVM function identified by name\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03mand wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolumn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _to_java_column\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(name, *(_to_java_column(col) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/functions/builtin.py:117\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03mInvokes n-ary JVM function identified by name\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03mand wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolumn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _to_java_column\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(name, *(\u001b[43m_to_java_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/classic/column.py:71\u001b[39m, in \u001b[36m_to_java_column\u001b[39m\u001b[34m(col)\u001b[39m\n\u001b[32m     69\u001b[39m     jcol = _create_column_from_name(col)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m     72\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN_OR_STR\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     73\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col).\u001b[34m__name__\u001b[39m},\n\u001b[32m     74\u001b[39m     )\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m jcol\n",
      "\u001b[31mPySparkTypeError\u001b[39m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got generator."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6d9f416-22a3-40fa-8a56-bd615a1d2f10",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reduce() missing 1 required positional argument: 'merge'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m docDf = docTFOrd.withColumn(\u001b[33m\"\u001b[39m\u001b[33mDF\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocCols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto/venv/lib/python3.12/site-packages/pyspark/sql/utils.py:282\u001b[39m, in \u001b[36mtry_remote_functions.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f.\u001b[34m__name__\u001b[39m)(*args, **kwargs)\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: reduce() missing 1 required positional argument: 'merge'"
     ]
    }
   ],
   "source": [
    "docDf = docTFOrd.withColumn(\"DF\", reduce(lambda a, b: a + b, (F.col(c) for c in docCols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be5deb4-822b-4fb3-9485-7df36068c663",
   "metadata": {},
   "source": [
    "docTFOrd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace69717-8e0a-455c-b2ab-0b0174d157d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15c14d4e-81c4-4b8c-b503-f534632dab70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+\n",
      "|    words|libro1|libro2|libro3|\n",
      "+---------+------+------+------+\n",
      "|     book|     0|     0|     1|\n",
      "| document|     1|     0|     0|\n",
      "|    first|     1|     0|     0|\n",
      "|    gonna|     0|     0|     1|\n",
      "|    hello|     1|     0|     0|\n",
      "| learnign|     0|     1|     0|\n",
      "| monsters|     0|     0|     1|\n",
      "|something|     0|     1|     0|\n",
      "|    start|     0|     1|     0|\n",
      "|teenagers|     0|     0|     1|\n",
      "|     want|     0|     1|     0|\n",
      "|     worl|     1|     0|     0|\n",
      "+---------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docTFOrd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31754616-d5f2-45e6-84aa-fddcccf98422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|nombre|    words|\n",
      "+------+---------+\n",
      "|libro1|    hello|\n",
      "|libro1|     worl|\n",
      "|libro1|    first|\n",
      "|libro1| document|\n",
      "|libro2|     want|\n",
      "|libro2|    start|\n",
      "|libro2| learnign|\n",
      "|libro2|something|\n",
      "|libro3|    gonna|\n",
      "|libro3|     book|\n",
      "|libro3| monsters|\n",
      "|libro3|teenagers|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtrado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3808f-c8a6-4312-804a-9e492d8c58b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
